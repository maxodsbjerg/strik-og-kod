{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/maxodsbjerg/strik-og-kod/blob/main/notebooks/SK_handson_notesbook_eng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/maxodsbjerg/strik-og-kod.git/main?labpath=notebooks%2FSK_handson_notesbook_eng.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByJpAFyOvr70"
   },
   "source": [
    "# Strik & Kod\n",
    "author: \"Max Odsbjerg Pedersen & Mathias Johansson\"\n",
    "\n",
    "date: \"2024-10-07\"\n",
    "\n",
    "This document consists of the coding part of the workshop \"Strik og Kod\" (Knit and Code) from AU Library at the Royal Danish Library. The workshop is about drawing parallels between knitting and coding. \"Coding\" is understood here as the connection between coding-based data processing and is therefore within the field of data science. As the workshop is made in the context of the Arts the example will regard text mining. When text mining the primary interest is pulling information out of large corpuses - which is the exact interest of many humanists.\n",
    "\n",
    "No recipe is complete without a picture of the final product as one of the first items. And this is no exception. The final result at the end of this document is the visualisation shown just under this paragraph. It shows the most frequently appearing words in old newspaper articles concerning knitting after all stopwords has been removed (it, that, to, and, in - words which bear no larger meaning).\n",
    "\n",
    "![](https://raw.githubusercontent.com/maxodsbjerg/strik-og-kod/refs/heads/main/notebooks/graphics/strikke_wordcloud.png)\n",
    "\n",
    "Knitting words and words which accompany them.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "## Downloading and installing Python packages\n",
    "\n",
    "We work in the programming language Python, it affords a lot of possibilities for working with statistic and graphical presentation of the results. Python works with libraries, these libraries add different functionalities to the Python functionalities.\n",
    "\n",
    "In this workshop the relevant packages are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vBjTYxQvr75",
    "outputId": "4fa23bab-5852-4908-d17e-3c7d7d17bc4f"
   },
   "outputs": [],
   "source": [
    "print(\"install and load libraries\")\n",
    "!python -m pip install pandas wordcloud matplotlib\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BGwm7VGvr77"
   },
   "source": [
    "## Data – articles about knitting\n",
    "\n",
    "The first thing we need is some text data. We will here use data from the Danish newspaper collection. The data is supplied by the Royal Danish Library experimental Newspaper-API. Interaction with the API builds on searches on Mediestream which is the Royal Danish Libraries platform searching in the newspaper collection and other. Before using the API it is a good idea to get acquainted with the expanded searching codes of Mediestream. To learn the search tips of Mediestream - <https://www2.statsbiblioteket.dk/mediestream/info/soegetips>\n",
    "\n",
    "You can also see the actual search code which is used here:\n",
    "<https://gist.github.com/maxodsbjerg/e2dd484d3c9dcaa9c422a861d6a93f6e>\n",
    "\n",
    "When you feel confident with limiting your searches with search codes you can use this interface to make calls to the Newspaper API:\n",
    "<http://labs.statsbiblioteket.dk/labsapi/api//api-docs?url=/labsapi/api/openapi.yaml> (Choose \"aviser(newspapers)/export/fields\")\n",
    "\n",
    "We have in this workshop prepared an API-call that makes the following search which will returns the matches as data:\n",
    "\n",
    "> strik\\* AND py:[1845 TO 1850] NOT familyId:(stcroixavisdvi OR sanctthomaetidendedvi)\n",
    "\n",
    "This search gives us articles from the collection in the period 1845 to 1850, that contains words which begins with “strik” (knit) and have all possible endings. And so, we matches such as: “strikke” (knit), “strikning” (knitting), “striktøj”(knit cloth) and “strikketøj” (knitwear). But we also get other words such as “strikt” (strict).\n",
    "\n",
    "Searches in Mediestream looks like this: ![](https://raw.githubusercontent.com/maxodsbjerg/strik-og-kod/refs/heads/main/pics/mediestream_strik.png)\n",
    "\n",
    "But the data that the API returns to us is available as files in CSV-format (Comma Separated Values). To gain access to the data the API returns a link. This link contains the file which is our data. For some the link will open the file in your browser and you will then be able to see something like this:\n",
    "\n",
    "![](https://raw.githubusercontent.com/maxodsbjerg/strik-og-kod/refs/heads/main/pics/api_strik.png)\n",
    "\n",
    "For others the link will download the csv-file to your computer. The most important thing that the API returns is a link to the raw data which matches our search. Without any unnecessary colouring or available interface that we can interact with as in the Mediestream-search above. The raw data can be loaded directly into R and we will afterwards be able to handle the data. Let us get our articles on knitting into R!\n",
    "\n",
    "### Load data\n",
    "\n",
    "First we use the URL we have gotten from the API to download the corpus-csv file to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1HWgvDCvr79",
    "outputId": "8788e654-59ed-4e2d-e6ad-cc388cad4121"
   },
   "outputs": [],
   "source": [
    "!wget -c -nc \"http://labs.statsbiblioteket.dk/labsapi/api/aviser/export/fields?query=strik%2A%20AND%20py%3A%5B1845%20TO%201850%5D%20NOT%20familyId%3A%28stcroixavisdvi%20OR%20sanctthomaetidendedvi%29&fields=link&fields=timestamp&fields=fulltext_org&fields=familyId&fields=lplace&max=-1&structure=header&structure=content&format=CSV\" -O corpus.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPpYXyFMvr8A"
   },
   "source": [
    "Then we use the function `read_csv` from the pandas library to load the file's contents into a DataFrame and keep that in memory under the variable name \"strik\".\n",
    "A DataFrame is comparable to spreadsheet in that it is a large matrix that stores data we work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYSueAQ2vr8B"
   },
   "outputs": [],
   "source": [
    "strik = pd.read_csv(\"corpus.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMZ2G_Esvr8D"
   },
   "source": [
    "This gives us a new Pandas DataFrame named “strik” and containing 7810 rows and 16 columns.\n",
    "\n",
    "What is especially interesting for us is the column “fulltext_org” – This is\n",
    "where the text from the articles is stored. At first the text will not be easy\n",
    "on the eyes as it is filled with errors, and it is here where you meet the\n",
    "first downside of working with old text: OCR-errors.\n",
    "\n",
    "To understand why these errors occur it is necessary to turn towards the\n",
    "digitalization. In this process the newspapers are photocopied (either from\n",
    "microfilm or from the original), afterwards a computer algorithm runs through\n",
    "the pages of the newspapers. The computer algorithm does two things:\n",
    "\n",
    "  1. Segmenting the articles – with other words the algorithm guesses which\n",
    "  body belongs to which headline.\n",
    "  2. Doing text recognition so that the text becomes digital and becomes\n",
    "  searchable. This is also called OCR (Optical Character Recognition).\n",
    "\n",
    "This algorithm has been developed with modern newspapers in mind and is\n",
    "therefore pretty precise when used on more recent newspapers (from 1910\n",
    "until today). If the algorithm is used on older material the quality of\n",
    "the digitalization dwindles. This is in part due to layout of older newspapers\n",
    "differ from modern layouts. One of the big problems are that the text\n",
    "recognition is bad. This is a result of the typeface used in old newspapers\n",
    "which used fraktur when pressing newspapers. Some will recognize the typeface\n",
    "as gothic letters or curly letters. ![](https://raw.githubusercontent.com/maxodsbjerg/strik-og-kod/refs/heads/main/pics/fraktur.png) Our hope here is\n",
    "that the data is so large that we can gather something interesting despite\n",
    "the OCR-errors.\n",
    "\n",
    "\n",
    "\n",
    "## The Text mining task\n",
    "\n",
    "\n",
    "First we will convert the text into lowercase and split it into words using [Regular expression](https://en.wikipedia.org/wiki/Regex).\n",
    "We store these `lists` of lowercase words in the DataFrame in the column `word` and we expand this column into a new dataframe where each word has its own row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofFT3Lzovr8F"
   },
   "outputs": [],
   "source": [
    "strik['word'] = strik.fulltext_org.apply(lambda x: re.findall(r'\\w+', x.lower()))\n",
    "strik_tidy = strik.explode('word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1_QcrU1vr8G"
   },
   "source": [
    "Let us just print out the new data frame to see how the tidytext format looks in practice. This is achieved by writing the name of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iORiX19zvr8I",
    "outputId": "fb300f1b-83c3-4482-dd9f-50db5c3b3b79"
   },
   "outputs": [],
   "source": [
    "strik_tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbMvJlVPvr8J"
   },
   "source": [
    "If we flip through the columns (with the little black arrow in the top-right corner) the last column will now be “word” which only contains single words.\n",
    "\n",
    "## Analysis\n",
    "\n",
    "### Wordcloud\n",
    "\n",
    "To get an overview of our dataset we will begin by counting the most used words in the article about knitting in the period 1845 to 1850:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qy62aN8avr8K",
    "outputId": "02ae2e3c-c1ef-46a1-9f52-356d1cd1c0ef",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter(strik_tidy.word.values).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIASMASdvr8L"
   },
   "source": [
    "<br> To no one’s surprise most frequent words in the dataset is the grammatical particles. One way to negate these words is by using a stopword list which can be used to remove unwanted words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxQgt-lTvr8M",
    "outputId": "1a795c39-cbda-494f-e5c6-5ea7b61bf980"
   },
   "outputs": [],
   "source": [
    "!wget \"https://gist.githubusercontent.com/maxodsbjerg/4d1e3b1081ebba53a8d2c3aae2a1a070/raw/e1f63b4c81c15bb58a54a2f94673c97d75fe6a74/stopord_18.csv\" -O stopord.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSiSK5ePvr8N"
   },
   "outputs": [],
   "source": [
    "stopord = pd.read_csv(\"stopord.csv\")['word'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aGkWL8mvr8N"
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "We will filter out all the stopwords !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i00WiV0Rvr8O",
    "outputId": "25cb0732-3149-4bab-e590-587b211b8f35",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def not_stopword(word):\n",
    "  return word not in stopord\n",
    "\n",
    "words = Counter(filter(not_stopword, strik_tidy.word.values)).most_common()\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpmR4_6avr8P"
   },
   "source": [
    "<br> We can already see quite a few interesting words. Something points\n",
    "towards a connection between maids that are seeking “condition” which back\n",
    "in the day meant a “service position” or a space of sorts. We can also see\n",
    "an OCR-error “eondition” and another spelling of condition, “kondition”.\n",
    "\n",
    "But a list is a little boring to look at. Could we perhaps create a\n",
    "beautiful wordcloud? Of course we can!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-n-LEhX9vr8P",
    "outputId": "57348a28-66b6-4de2-fd59-f97cc052d74b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wc = WordCloud()\n",
    "wc.generate_from_frequencies(\n",
    "  Counter(filter(not_stopword, strik_tidy.word.values))).to_file('wc.png')\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTLUwFB3vr8R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
