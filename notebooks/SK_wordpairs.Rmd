---
title: "Strik & Kod - Ordpar"
author: "Max Odsbjerg Pedersen"
date: "1/28/2022"
bibliography: references.bib
no-cite: '@tidyverse, @tidytext, @igraph'
output: 
    html_document:
      df_print: paged
      toc: true
      toc_depth: 2
      toc_float: true
---

```{=html}
<style>
    body { background-color: white; }
    p {color:black;}
    h1,h2,h3,h4,h5,h6 {color:#f7a1bd;}
</style>
```
Dette dokument udgør kodedelen af workshoppen Strik og Kod fra AU Library, Det Kgl. Bibliotek. Workshoppen handler om at trække paralellerne mellem strikning og kodning. "Kodning" forståes i denne sammenhæng som kodebaseret databehandling og lægger sig derfor inden for feltet data science. Som følge af workshoppens afsæt på Arts vil det nærmere blive et text mining eksempel. I text mining er man interesseret i at udtrække informationer af store mængder tekst - hvilket netop interesserer de fleste humanister.

Ingen kageopskrift uden et billede af kagen som noget af det første. Det samme gør sig gældende her. Det endelige resultat for enden af dette dokument er visualiseringen herunder, der viser hvilke ord, der bruges før forskellige ord om strikning. I grafen kan vi se at der går en pil fra "bomulds" og peger mod "strikkegarn", hvilket betyder, at det har optrådt som "bomulds garn". Jo mere markant pilen er jo flere forekomster har der været. Således kan vi altså se at et meget hyppigt forekommende ordpar er "di strikt", hvilken tydeligvis er en fejllæsning af ordet "distrikt". Dette handler om at vores data er maskingenkendt, hvilket ikke altid går lige godt. Mere om det senere.

# ![](strikke_bigrams.png) 

<br>

# Indlæsning af R-pakker

Vi arbejder i statistik-programmet R, der giver mange muligheder for statistisk arbejde og efterfølgende grafisk fremstilling af resultaterne. I R arbejder man med pakker, som tilføjer forskellige funktionaliteter til grundstammen af R-funktioner. I dette tilfælde er de relevante pakker:

```{r, message=FALSE}
library(tidyverse)
library(tidytext)
```

# Data - artikler om strikning

Det første vi har brug for er noget tekstdata. Her vil vi bruge data fra den danske avissamling. Data vil blive leveret af Det Kgl. Biblioteks eksperimentielle Newspaper-API. Interaktion med API'en bygger på søgninger i Mediestream, der er Det Kgl. Biblioteks platform til søgning blandt andet i avissamlingen. Inden man kaster sig over API'en er det en kod idé at gøre sig bekendt med de udvidede søgekoder til Mediestream. Hertil kan man bruge Mediestream søgetips - <https://www2.statsbiblioteket.dk/mediestream/info/soegetips>

Se desuden konkrete søgekoder i spil her: <https://gist.github.com/maxodsbjerg/e2dd484d3c9dcaa9c422a861d6a93f6e>

Når man er tryg ved at afgrænse sine søgninger med søgekoder kan man bruge dette interface til at lave sine kald til Newspaper API'et: <http://labs.statsbiblioteket.dk/labsapi/api//api-docs?url=/labsapi/api/openapi.yaml> (Vælg "aviser/export/fields")

I denne workshop har vi forberedt et API-kald der laver følgende søgning og returnerer det matchede som data:

> strik\* AND py:[1845 TO 1850]

Denne søgning giver os altså artikler i avissamlingen i perioden 1840 til 1850, som indeholder ord der starter med "strik" og alle endelser. Derved får vi både "strikke", "strikning", "strikkede", "striktøj" og "strikketøj". Men vi får også ældre ord som "strikt" med.

I Mediestream ser søgningen således ud: ![](pics/mediestream_strik.png) Men når vi får API til at returnere data for os, så får vi den til at returnere det i CSV-format(Comma Separated Values). Den måde API'en returnerer data er ved at give os et link. Dette link vil indeholde filen med vores data. I nogle vil linket åbne filen i browseren og det vil se nogenlunde sådan her ud:

![](pics/api_strik.png) I andre tilfælde vil den downloade csv-filen ned på din computer. Det vigste er i midlertidig at API giver os et link med de rå data der passer til vores søgning. Ikke noget farvelade og interface vi kan pege og klikke på som i Mediestream-søgningen ovenfor. Det rå data kan puttes direkte ind i R, hvorefter man begynder at kunne lave sin databehandling. Lad os få vores artikler om strikning ind i R!

## Indlæs data

I kodeboksen herunder bruger vi functionen `read_csv` til at læse det link vi har fået fra API'en. Denne læsning gemmer vi i et element, som vi kalder "strik"(det kunne også være "strikke_artikler_1840_1850", men vi kommer til at referere til det mange gange fremefter, så det er fedt, at det er kort og præcist):

```{r}
strik <- read_csv("http://labs.statsbiblioteket.dk/labsapi/api/aviser/export/fields?query=strik%2A%20AND%20py%3A%5B1845%20TO%201850%5D&fields=link&fields=recordID&fields=timestamp&fields=pwa&fields=cer&fields=fulltext_org&fields=pageUUID&fields=editionUUID&fields=titleUUID&fields=editionId&fields=familyId&fields=newspaper_page&fields=newspaper_edition&fields=lplace&fields=location_name&fields=location_coordinates&max=-1&structure=header&structure=content&format=CSV")
```

Dette giver os en ny data frame i det panel der hedder "Environment" - Den hedder "strik" og vi kan se, at den har 7810 observation af 16 variable. Klik på "strik"-data framen og inspicer dit data!

Særlig interessant for os er kolonnen "fulltext_org" - det er her teksten fra artiklerne bor. Ved første øjekast skærer det dog i øjnene. Den er sprængfyldt med fejl og her er i stødt på en af faldgruberne ved at arbejde med gammel tekst: OCR-fejl.

For at forstå hvorfor disse fejl er opstået er det nødvendigt at vende blikket mod digitaliseringen. I denne proces affotograferer man aviserne (enten fra mikrofilm eller fra original), Herefter lader man en computeralgoritme løbe igennem avissiderne. Denne algoritme gør to ting: 1. Segmenterer artiklerne - med andre ord så gætter den hvilke rubrikker hører til hvilke overskrifter 2. Udfører tekstgenkendelse således, at teksten bliver digital og man kan søge i den. Dette kaldes også OCR (Optical Character Recognition)

Denne algoritme er udviklet til moderne aviser, og derfor er resultatet oftest ret godt, når man har med nyere aviser at gøre (1910 til nu). Går man længere tilbage i tiden, begynder kvaliteten på digitaliseringen at falde. Dette skyldes blandt andet, at opsætningen af aviser er en anelse forskellige fra moderne opsætning. Èn af de helt store problemer er, at tekstgenkendelsen er dårlig. Dette skyldes, at man i gamle aviser brugte frakturtyper til at trykke sine aviser. Denne tekst vil nogle kende som gotiske bogstaver eller krøllede bogstaver. ![](pics/fraktur.png) Håbet er dog imidlertidigt at vores data er så omfavnrigt, at vi alligevel kan få noget spændende ud af det.

# Indledende analyse - og lidt oprensning 

En anden af kolonner er "lplace" som angiver hvor avisen er udgivet henne. Ved først at angive hvilken data frame som vi arbejder med. Dernæst bruger vi den såkaldte pipe, `%>%`, til at sende data videre til en funktion. Her bruger vi `count`, der skal have vide hvilken kolonne den skal tælle sammen. Vi angiver her at vi vil tælle på "lplace", der ved kan vi se hvordan vores materiale fordeler sig geografisk:

```{r}
strik %>% 
  count(lplace, sort = TRUE)
```

Ovenfor kan vi se optællingenen og ikke overraskende er langt den største del af materialet fra København, der i perioden var klart den største by med flest aviser. Der dukker dog også andre interessante ting op. "Charlote Amalie" og "Christianssted" er byer på det daværende Dansk Vestindien hvorfor de altså er i avissamlingen. Disse er mestendels på engelsk og vil forstyrre i text miningen af de danske artikler om strikning. Derfor vil vi sortere dem fra, men inden vi kaster os over det, så ser vi også at Aarhus både benævnes som "Århus" og som "Aarhus". Disse to skal vi også have justeret således, at vi kun har en Aarhus i datasættet.

Lige som før starter vi med at nævne den data frame vi er interesseret i at arbejde med. Med pipen `%>%` fører vi det videre til `filter`, der filtrerer data på lplace kolonnen. Bid mærke i udråbstegnet(`!`) foran "lplace"!!!! Dette gør at den filtrere Christianssted og Charlotte Amalie fra. Uden udråbstegnet ville vi kun få tekst fra de to byer!

Dernæst piper(`%>%`) vi data videre (nu uden de to Danske Vestindiske byer) til en ny funktion. Det er en god idé at tænke på pipen som netop et rør, som man hælder data ned i, hvorefter det gribes af en funktion, der laver en ændring på data og resultatet af denne ændring kan så pipes videre igen. Den nye funktion er `mutate`, der laver ændringer på værdier i kolonner eller skaber nye kolonner. Her vil vi gerne ændre på kolonne "lplace" og den ændring får vi funktionen `str_replace` til at lave. Funktionen skal læses og siges som "string replace" og i den sammenhæng skal "string" forståes som en karakterstreng. Den finder altså en karakterstreng og erstatter den med en anden. I vores sammenhæng erstatter den "Århus" med "Aarhus".

Til sidst gemmer vi ændringer til vores dataframe "strik". Det sker til allersidst med `-> strik`. Hvis ikke man gør dette "falder" resultatet ud under kodeboksen, som vi så ovenfor, da vi talte "lplace".

```{r}
strik %>% 
  filter(!lplace %in% c("Christianssted", "Charlotte Amalie")) -> strik
```

Vi kan nu igen tælle lplace og se, at ændringerne er som vi vil have dem (bid mærke i at resultatet falder ud under boksen, da vi *ikke* har angivet `->`, som gemmer resultatet til en nyt element (ligesom vi gjorde før med "strik"):

```{r}
strik %>% 
  count(lplace, sort = TRUE)
```

# Text mining opgaven - N-grams

Data behandlingen vil tage udgangspunkt i [Tidy Data-princippet](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) som den er implementeret i tidytext-pakken. Tankegangen er her at tage en tekst og splitte den op i mindre dele. Den typiske tilgang er at splitte teksten op i enkelte ord. På denne måde optræder der kun ét ord per række i datasættet. Men man kan også splitte teksten op i ordpar(eller ordtrioer, ordkvartetter osv.) Dette kaldes i text mining verdenen N-grams, da man i princippet kan lave sekvenser af præcis så mange ord, som man vil. Når man har med ordpar at gøre så kaldes de bigrams. 

# Bigrams
N-grams er overlappende så i et scenarie med bigrams bliver teksten "den glade kat går ad tagryggen" til:

"den glade", "glade kat", "kat går","går ad","ad tagryggen", "tagryggen NA"

Bemærk at det sidste ord i det sidste bigram er værdien "NA". Der er altså ikke noget sidste ord i det bigram.

Ligesom før bruger vi `unnest_tokens`, men denne gang specificerer vi at vi vil have ordpar(bigrams):

```{r}
strik %>% 
  unnest_tokens(bigram, fulltext_org, token = "ngrams", n = 2) -> strik_bigrams
```

Lad os se det in action. Istedet for blot at skrive navnet på vores nye data frame og bladre i kolonnerne bruger vi nu pipen og funktionen `select` til kun at vælge vores nye kolonne:

```{r}
strik_bigrams %>% 
  select(bigram)
```

Lige som vi kunne med lplace, så kan vi også optælle bigrams:

```{r}
strik_bigrams %>% 
  count(bigram, sort = TRUE)
```
<br> Allerede her har vi en del interessante ordpar. Men der er noget der tyder på, at der er en del sammenhæng mellem tjenestefolk, der søger "condition", som i gammeldags sprogbrug er "tjenende stilling" eller en plads. Vi ser også OCR-fejlen "eondition" og den anden stavemåde "kondition".

Desuden støder vi på stopord der forstyrer os. Ordpar med stopord kunne vi godt tænke os at sortere fra. Først skal vi dog have indlæst en stopordsliste:

```{r message=FALSE}
stopord <- read_csv("https://gist.githubusercontent.com/maxodsbjerg/4d1e3b1081ebba53a8d2c3aae2a1a070/raw/e1f63b4c81c15bb58a54a2f94673c97d75fe6a74/stopord_18.csv")
```

<br>

Før vi kan fjerne ordpar hvor et af ordene er stopord, er vi dog nødt til at have splittet kolonnen "bigram" op i to: "word1", "word2":

```{r}
strik_bigrams %>% 
  separate(bigram, c("word1", "word2"), sep = " ") ->strik_bigrams_separated
```

Derefter kan vi filtrere stopordene ud i begge kolonner, hvilket vi gemmer til en ny dataframe:

```{r}
strik_bigrams_separated %>% 
  filter(!word1 %in% stopord$word) %>%
  filter(!word2 %in% stopord$word) -> strik_bigrams_filtered
```

Dernæst kan vi optælle vores bigrams uden stopord

```{r}
strik_bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)
```
Det er nu helt tydeligt at det meget handler om tjenesteforhold. I særdeles folk der ønsker ansættelses. Men hvordan forholder det sig helt konkret med strikkeord? Hvilke ord bruges foran dem?

Eftersom vi har bigram i to kolonner kan vi nu også styre præcis hvilket ord vi kigger på som ord nummer 2. Lad os prøve med "strikke-ord". Tricket her er funktionen `str_detect`, som får at vide at den leder ord der starter med "strik" og kan efterfølges af 0 eller flere bogstaver mellem a til z og æ og ø. "\\b" angiver at det efterfølgende s skal være starten af ordet. Denne måde at angive tekstmønstre på kaldes regulære udtryk og er en kraftfuld og avanceret måde at søge efter mønstre i tekst.

```{r}
strik_bigrams_filtered %>% 
  filter(str_detect(word2, "\\bstrik[a-zæø]*")) %>% 
  count(word1, word2, sort = TRUE)
```

Vi ser stadig at "di strikt" spøger en smule, men der dukker pludselig en masse interessante bigrams op. En måde at visualisere det bedre på end en liste er gennem en netværks-graf. På listen oven for ses at flere af de hyppigt forekommende ordpar har "strikkegarn" som word2. I en netværksgraf vil strikkegarn altså blive et punkt, mens "uldent", "bomulds", "coul, og "couleurt" vil være punkter der peger ind mod "strikkegarn". På denne måde kan man på en ret overskuelig måde illustrere flere ords interne forhold.

Allerførst gemmer vi den ovenstående optælling til en ny data frame, så vi kan arbejde videre med den:

```{r}
strik_bigrams_filtered %>% 
  filter(str_detect(word2, "\\bstrik[a-zæø]*")) %>% 
  count(word1, word2, sort = TRUE) -> strikkeord_bigrams_counts
```

Herefter bruger vi biblioteket "igraph" til at lave vores dataframe om til et netværksgraf-element. Inden da specificerer vi, at vi kun er interesserede i bigrams, der optræder mere en 8 gange:

```{r, message=FALSE}
library(igraph)

bigram_graph <- strikkeord_bigrams_counts %>%
  filter(n > 8) %>%
  graph_from_data_frame()
```

Tilsidst bruger vi pakken "ggraph" til at visualisere netværket:

```{r}
library(ggraph)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "#f7a1bd", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

Herved for vi altså på en overskuelig måde visualiseret de forskellige ords forhold. 

For at gemme grafen bruger vi funktionen `ggsave`, hvor man angiver filnavn og type efterfulgt af bredde og højde og hvilken enhed, samt baggrundsfarven.

```{r}
ggsave("graphics/strikke_bigrams.png", width = 28, height = 20, units = "cm", bg = "white")
```

# Fra distant reading til close reading

Okay der er noget med en ny strikkebog. Hvordan går vi fra denne "distant reading"-indsigt frem til at, hvad der rent faktisk sker med den nye strikkebog. Altså bevægelsen fra distant reading til klassisk humanistisk nærlæsning. Gennem et par filtre kan vi faktisk ret hurtigt få links indtil Mediestream hvor man er tilbage helt tæt ved kilden:

```{r}
strik %>% 
  filter(str_detect(fulltext_org, regex("nyeste strikkebog", ignore_case = TRUE))) %>% 
  select(fulltext_org, timestamp, link)
```

(Psst nr. 2 og dernæst nr 5 er interessante og sjove)

Følgende dokument er baseret på [Tidy Text Mining with R](https://www.tidytextmining.com)-bogen. Særligt kapitlerne 1 om tidyformatet generelt og 4 om n-grams. Bogen kan varmt anbefales til videre læsning og inspiration til andre undersøgelser som text mining kan.

# References
